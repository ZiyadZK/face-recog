{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "61be7e09-e1a6-4ba5-bce8-45beacd06f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4c83cde1-c50d-4ab5-8354-8d4efd9662e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membuat model kontrastif\n",
    "def create_contrastive_model(input_shape, embedding_size=64):\n",
    "    input_a = tf.keras.Input(shape=input_shape)\n",
    "    input_b = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Bagian jaringan konvolusi bersama\n",
    "    shared_conv = models.Sequential([\n",
    "        layers.Conv2D(32, (5, 5), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (5, 5), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (5, 5), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(embedding_size, activation='relu')\n",
    "    ])\n",
    "\n",
    "    # Menghasilkan representasi vektor untuk setiap input\n",
    "    output_a = shared_conv(input_a)\n",
    "    output_b = shared_conv(input_b)\n",
    "\n",
    "    # Layer untuk menghitung jarak Euclidean antara dua vektor\n",
    "    distance = layers.Lambda(lambda embeddings: tf.norm(embeddings[0] - embeddings[1], axis=1, keepdims=True), name='distance')([output_a, output_b])\n",
    "\n",
    "    # Membangun model\n",
    "    model = Model(inputs=[input_a, input_b], outputs=distance)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d714388b-2cd4-4afc-8e2d-f3174fb17889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "15bc1515-a043-4c88-b52d-6d70838dda6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (160, 160, 1)  # Sesuaikan dengan ukuran gambar yang digunakan\n",
    "contrastive_model = create_contrastive_model(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "61afb9aa-f1ee-4bae-935e-c0dbbff7d214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, distance):\n",
    "    margin = 1.0\n",
    "    return tf.reduce_mean(y_true * tf.square(distance) + (1 - y_true) * tf.square(tf.maximum(margin - distance, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "25808695-d5ae-4ba7-8ea8-dd4365cc877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "contrastive_model.compile(optimizer='adam', loss=contrastive_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6f95c765-823d-4953-b00e-5a69f8bc2353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_41\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_53 (InputLayer)       [(None, 160, 160, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " input_54 (InputLayer)       [(None, 160, 160, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " sequential_11 (Sequential)  (None, 64)                   2354240   ['input_53[0][0]',            \n",
      "                                                                     'input_54[0][0]']            \n",
      "                                                                                                  \n",
      " distance (Lambda)           (None, 1)                    0         ['sequential_11[0][0]',       \n",
      "                                                                     'sequential_11[1][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2354240 (8.98 MB)\n",
      "Trainable params: 2354240 (8.98 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "contrastive_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "edd61ea3-0228-44f5-b36e-44c826f9de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_model.save('facematching.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9b7998fc-a83e-4e8a-b610-c434a0b09f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "custom_objects = {'contrastive_loss': contrastive_loss}\n",
    "model = load_model('facematching.h5', custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "913941b9-9ace-4837-bd54-0a2f46d0f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4f164e80-f9bc-44d9-a7d2-517b18a8a51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FACELOADING:\n",
    "    def __init__(self):\n",
    "        self.target_size = (160, 160)  # Adjusted target size for the grayscale images\n",
    "        self.detector = MTCNN()\n",
    "\n",
    "    def extract_face(self, img):\n",
    "        # Convert BGR to RGB\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect faces\n",
    "        results = self.detector.detect_faces(img_rgb)\n",
    "        \n",
    "        # Check if a face is detected\n",
    "        if results:\n",
    "            # Get the first face (assuming there is only one)\n",
    "            x, y, w, h = results[0]['box']\n",
    "            \n",
    "            # Crop the face region\n",
    "            cropped_face = img[y:y+h, x:x+w]\n",
    "            \n",
    "            # Convert to grayscale and resize\n",
    "            cropped_face = cv2.cvtColor(cropped_face, cv2.COLOR_BGR2GRAY)\n",
    "            cropped_face = cv2.resize(cropped_face, self.target_size)\n",
    "            \n",
    "            return cropped_face\n",
    "        else:\n",
    "            # Return None if no face is detected\n",
    "            return None\n",
    "\n",
    "    def load_faces(self, dir):\n",
    "        faces = []\n",
    "        for im_name in os.listdir(dir):\n",
    "            try:\n",
    "                path = os.path.join(dir, im_name)\n",
    "                img = cv2.imread(path)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                single_face = self.extract_face(img)\n",
    "                if single_face is not None:\n",
    "                    faces.append(single_face)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        return faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2539ed3b-f5d2-415b-90c9-9c9e4d8574a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "03d87640-2eca-4278-b207-7ea21439d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = FACELOADING()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95648791-4c54-4604-9a16-31bf3f7d8048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "08edf005-6631-4f5d-8958-563dfb08b17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "3/3 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "3/3 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n"
     ]
    }
   ],
   "source": [
    "base_images_dir = 'data/base_image/ojiie'\n",
    "base_images = loader.load_faces(base_images_dir)\n",
    "base_images = np.array(base_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22291f1c-e66f-496a-869a-c4282db59c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "06a9180b-9076-454f-ae1e-6308b41df6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of base_images: (3, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of base_images: {base_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "aff8c2c6-e506-4497-a8e7-f01a3a016664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    }
   ],
   "source": [
    "verif_image_dir = 'data/verif_image/aaang'\n",
    "verif_image = loader.load_faces(verif_image_dir)\n",
    "verif_image = np.array(verif_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e226b043-1836-4c82-9af4-ac2696b5fbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of verif_images: (3, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of verif_images: {verif_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d8baa003-3fcd-4207-a372-fa16c813760b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 135ms/step\n",
      "Prediction: 33.29273223876953\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Perform prediction\n",
    "prediction = model.predict([base_images, verif_image])\n",
    "\n",
    "# Display the prediction\n",
    "print(f'Prediction: {prediction[0][0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df807b9e-3dce-476f-b4c6-3747d932f833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74000253-b83a-4a24-becf-3248b9a8aba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
