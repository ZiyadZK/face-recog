{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "996a5cb1-d5c4-4661-8678-284b54216c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc4d374-8127-4f14-b45a-859df1f77ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for custom loss\n",
    "def contrastive_loss(y_true, distance):\n",
    "    margin = 1.0\n",
    "    return tf.reduce_mean(y_true * tf.square(distance) + (1 - y_true) * tf.square(tf.maximum(margin - distance, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8220765-d87c-4000-aa07-765d4fbb1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the model and put custom loss inside\n",
    "custom_objects = {'contrastive_loss': contrastive_loss}\n",
    "model = load_model('facematching.h5', custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9f0e0bb-73cb-43e5-9131-626c47a0bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for preprocessing image\n",
    "class FACELOADING:\n",
    "    def __init__(self):\n",
    "        self.target_size = (160, 160)  # Adjusted target size for the grayscale images\n",
    "        self.detector = MTCNN()\n",
    "\n",
    "    def extract_face(self, img):\n",
    "        # Convert BGR to RGB\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect faces\n",
    "        results = self.detector.detect_faces(img_rgb)\n",
    "        \n",
    "        # Check if a face is detected\n",
    "        if results:\n",
    "            # Get the first face (assuming there is only one)\n",
    "            x, y, w, h = results[0]['box']\n",
    "            \n",
    "            # Crop the face region\n",
    "            cropped_face = img[y:y+h, x:x+w]\n",
    "            \n",
    "            # Convert to grayscale and resize\n",
    "            cropped_face = cv2.cvtColor(cropped_face, cv2.COLOR_BGR2GRAY)\n",
    "            cropped_face = cv2.resize(cropped_face, self.target_size)\n",
    "            \n",
    "            return cropped_face\n",
    "        else:\n",
    "            # Return None if no face is detected\n",
    "            return None\n",
    "\n",
    "    def load_faces(self, dir):\n",
    "        faces = []\n",
    "        for im_name in os.listdir(dir):\n",
    "            try:\n",
    "                path = os.path.join(dir, im_name)\n",
    "                img = cv2.imread(path)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                single_face = self.extract_face(img)\n",
    "                if single_face is not None:\n",
    "                    faces.append(single_face)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        return faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d84f690-0394-435c-aaf3-44f68639f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = FACELOADING()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37d09adb-177c-4c41-b0d2-3bd06a84c0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 721ms/step\n",
      "1/1 [==============================] - 0s 482ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 229ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "3/3 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "3/3 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n"
     ]
    }
   ],
   "source": [
    "# preprocess base images, the count of base image should be the same with verif image\n",
    "base_images_dir = 'data/base_image/ojiie'\n",
    "base_images = loader.load_faces(base_images_dir)\n",
    "base_images = np.array(base_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29d88658-7bb3-4e7e-86b1-c2b6d9fed2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of base_images: (3, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "# make sure the shape of images\n",
    "print(f\"Shape of base_images: {base_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "628571e0-c993-40fd-8ef7-5354b686d8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n"
     ]
    }
   ],
   "source": [
    "# preprocess base images, the count of base image should be the same with verif image\n",
    "verif_image_dir = 'data/verif_image/ojiie'\n",
    "verif_image = loader.load_faces(verif_image_dir)\n",
    "verif_image = np.array(verif_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29ac3ceb-9c4a-4cfd-8309-d0e3b96a520b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of verif_images: (3, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "# make sure the shape of images\n",
    "print(f\"Shape of verif_images: {verif_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c039a27c-f2a7-4eea-aad6-f599f204e725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 328ms/step\n",
      "Prediction: 15.535365104675293\n"
     ]
    }
   ],
   "source": [
    "# Perform prediction if the images matching\n",
    "prediction = model.predict([base_images, verif_image])\n",
    "\n",
    "# Display the prediction\n",
    "print(f'Prediction: {prediction[0][0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1053d3c-49e2-4059-8491-18ac0ebb8c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
